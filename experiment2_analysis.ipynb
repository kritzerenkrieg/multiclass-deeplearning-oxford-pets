{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c84b749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import f1_score\n",
    "from pathlib import Path\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "img_size = (128, 128)\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "num_classes = 5\n",
    "l2_lambda = 0.01\n",
    "k_folds = 5\n",
    "\n",
    "base_dir = Path(\"splitted_data_kfold\")\n",
    "results = []\n",
    "\n",
    "# === DATA AUGMENTATION ===\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ddb0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL BUILDER ===\n",
    "def build_model():\n",
    "    model = models.Sequential([\n",
    "        # Layer 1\n",
    "        layers.Conv2D(32, (3,3), activation='relu', input_shape=img_size + (3,)),\n",
    "        layers.MaxPooling2D((2,2), strides=2),\n",
    "\n",
    "        # Layer 2\n",
    "        layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        layers.MaxPooling2D((2,2), strides=2),\n",
    "        layers.Dropout(0.2),\n",
    "\n",
    "        # Layer 3\n",
    "        layers.Conv2D(128, (3,3), activation='relu'),\n",
    "        layers.AveragePooling2D((2,2), strides=2),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_lambda)),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55f0e3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Fold 1/5\n",
      "Found 800 images belonging to 5 classes.\n",
      "Found 200 images belonging to 5 classes.\n",
      "Class indices: {'American_Bulldog': 0, 'German_Shorthaired': 1, 'Havanese': 2, 'Maine_Coon': 3, 'Pomeranian': 4}\n",
      "American_Bulldog : 160\n",
      "German_Shorthaired : 160\n",
      "Havanese : 160\n",
      "Maine_Coon : 160\n",
      "Pomeranian : 160\n",
      "Epoch 1/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 298ms/step - accuracy: 0.2537 - loss: 2.3740 - val_accuracy: 0.2850 - val_loss: 1.7452\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 287ms/step - accuracy: 0.3525 - loss: 1.5739 - val_accuracy: 0.4300 - val_loss: 1.5436\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 303ms/step - accuracy: 0.4563 - loss: 1.4382 - val_accuracy: 0.4500 - val_loss: 1.5085\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 310ms/step - accuracy: 0.5163 - loss: 1.3498 - val_accuracy: 0.5050 - val_loss: 1.4911\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 296ms/step - accuracy: 0.5400 - loss: 1.3164 - val_accuracy: 0.4550 - val_loss: 1.4632\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 291ms/step - accuracy: 0.5600 - loss: 1.2335 - val_accuracy: 0.4550 - val_loss: 1.6586\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 287ms/step - accuracy: 0.5962 - loss: 1.1880 - val_accuracy: 0.4600 - val_loss: 1.5410\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 298ms/step - accuracy: 0.6000 - loss: 1.2343 - val_accuracy: 0.4400 - val_loss: 1.7504\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 294ms/step - accuracy: 0.6425 - loss: 1.1407 - val_accuracy: 0.5100 - val_loss: 1.7469\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 306ms/step - accuracy: 0.6475 - loss: 1.1064 - val_accuracy: 0.4600 - val_loss: 1.9350\n",
      "Fold 1 Results: accuracy=0.4550, loss=1.4632, f1=0.4356\n",
      "\n",
      "🔹 Fold 2/5\n",
      "Found 800 images belonging to 5 classes.\n",
      "Found 200 images belonging to 5 classes.\n",
      "Class indices: {'American_Bulldog': 0, 'German_Shorthaired': 1, 'Havanese': 2, 'Maine_Coon': 3, 'Pomeranian': 4}\n",
      "American_Bulldog : 160\n",
      "German_Shorthaired : 160\n",
      "Havanese : 160\n",
      "Maine_Coon : 160\n",
      "Pomeranian : 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 315ms/step - accuracy: 0.1963 - loss: 2.5348 - val_accuracy: 0.2000 - val_loss: 1.9202\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 295ms/step - accuracy: 0.2562 - loss: 1.7738 - val_accuracy: 0.3150 - val_loss: 1.6517\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 305ms/step - accuracy: 0.2950 - loss: 1.6146 - val_accuracy: 0.3650 - val_loss: 1.5619\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 298ms/step - accuracy: 0.3900 - loss: 1.4898 - val_accuracy: 0.4150 - val_loss: 1.4611\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 296ms/step - accuracy: 0.4300 - loss: 1.4343 - val_accuracy: 0.4650 - val_loss: 1.3768\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 299ms/step - accuracy: 0.5013 - loss: 1.3170 - val_accuracy: 0.4600 - val_loss: 1.4098\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 323ms/step - accuracy: 0.5325 - loss: 1.2557 - val_accuracy: 0.4850 - val_loss: 1.3701\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 328ms/step - accuracy: 0.5750 - loss: 1.1887 - val_accuracy: 0.5050 - val_loss: 1.3099\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 337ms/step - accuracy: 0.5875 - loss: 1.1374 - val_accuracy: 0.5000 - val_loss: 1.4283\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 345ms/step - accuracy: 0.6212 - loss: 1.0995 - val_accuracy: 0.4550 - val_loss: 1.5072\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 314ms/step - accuracy: 0.6112 - loss: 1.1150 - val_accuracy: 0.4200 - val_loss: 1.7964\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 309ms/step - accuracy: 0.6400 - loss: 1.1064 - val_accuracy: 0.4800 - val_loss: 1.5383\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 316ms/step - accuracy: 0.6775 - loss: 1.0025 - val_accuracy: 0.5250 - val_loss: 1.4015\n",
      "Fold 2 Results: accuracy=0.5050, loss=1.3099, f1=0.4923\n",
      "\n",
      "🔹 Fold 3/5\n",
      "Found 800 images belonging to 5 classes.\n",
      "Found 200 images belonging to 5 classes.\n",
      "Class indices: {'American_Bulldog': 0, 'German_Shorthaired': 1, 'Havanese': 2, 'Maine_Coon': 3, 'Pomeranian': 4}\n",
      "American_Bulldog : 160\n",
      "German_Shorthaired : 160\n",
      "Havanese : 160\n",
      "Maine_Coon : 160\n",
      "Pomeranian : 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 363ms/step - accuracy: 0.2262 - loss: 2.4292 - val_accuracy: 0.1950 - val_loss: 1.8203\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 328ms/step - accuracy: 0.2237 - loss: 1.7052 - val_accuracy: 0.2950 - val_loss: 1.6362\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 319ms/step - accuracy: 0.2788 - loss: 1.5933 - val_accuracy: 0.3950 - val_loss: 1.4723\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 300ms/step - accuracy: 0.3775 - loss: 1.4912 - val_accuracy: 0.3900 - val_loss: 1.4500\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 327ms/step - accuracy: 0.4200 - loss: 1.4210 - val_accuracy: 0.4100 - val_loss: 1.4777\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 333ms/step - accuracy: 0.4487 - loss: 1.3851 - val_accuracy: 0.3800 - val_loss: 1.4950\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 326ms/step - accuracy: 0.4963 - loss: 1.3630 - val_accuracy: 0.4700 - val_loss: 1.3782\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 308ms/step - accuracy: 0.4650 - loss: 1.3562 - val_accuracy: 0.4600 - val_loss: 1.4761\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 286ms/step - accuracy: 0.5425 - loss: 1.2649 - val_accuracy: 0.3900 - val_loss: 1.7206\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 284ms/step - accuracy: 0.5913 - loss: 1.1842 - val_accuracy: 0.5150 - val_loss: 1.4260\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 290ms/step - accuracy: 0.5875 - loss: 1.1723 - val_accuracy: 0.4800 - val_loss: 1.3112\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 313ms/step - accuracy: 0.6112 - loss: 1.1445 - val_accuracy: 0.4800 - val_loss: 1.4820\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 301ms/step - accuracy: 0.6288 - loss: 1.1047 - val_accuracy: 0.5250 - val_loss: 1.5254\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 311ms/step - accuracy: 0.6413 - loss: 1.0534 - val_accuracy: 0.5150 - val_loss: 1.5198\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 307ms/step - accuracy: 0.6825 - loss: 0.9395 - val_accuracy: 0.4600 - val_loss: 1.7011\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 306ms/step - accuracy: 0.6762 - loss: 0.9820 - val_accuracy: 0.5100 - val_loss: 1.4178\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001FD66353F40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001FD66353F40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Results: accuracy=0.4800, loss=1.3112, f1=0.4546\n",
      "\n",
      "🔹 Fold 4/5\n",
      "Found 800 images belonging to 5 classes.\n",
      "Found 200 images belonging to 5 classes.\n",
      "Class indices: {'American_Bulldog': 0, 'German_Shorthaired': 1, 'Havanese': 2, 'Maine_Coon': 3, 'Pomeranian': 4}\n",
      "American_Bulldog : 160\n",
      "German_Shorthaired : 160\n",
      "Havanese : 160\n",
      "Maine_Coon : 160\n",
      "Pomeranian : 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 350ms/step - accuracy: 0.2013 - loss: 2.6688 - val_accuracy: 0.2000 - val_loss: 1.9988\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 314ms/step - accuracy: 0.2837 - loss: 1.8135 - val_accuracy: 0.3100 - val_loss: 1.7040\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 319ms/step - accuracy: 0.3925 - loss: 1.5800 - val_accuracy: 0.4700 - val_loss: 1.5283\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 305ms/step - accuracy: 0.4575 - loss: 1.4243 - val_accuracy: 0.4800 - val_loss: 1.4027\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 309ms/step - accuracy: 0.5063 - loss: 1.3343 - val_accuracy: 0.4500 - val_loss: 1.5185\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 292ms/step - accuracy: 0.5437 - loss: 1.2630 - val_accuracy: 0.5000 - val_loss: 1.3669\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 305ms/step - accuracy: 0.5888 - loss: 1.2197 - val_accuracy: 0.4250 - val_loss: 1.5075\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 306ms/step - accuracy: 0.5975 - loss: 1.1788 - val_accuracy: 0.5000 - val_loss: 1.4567\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 292ms/step - accuracy: 0.6175 - loss: 1.1352 - val_accuracy: 0.5650 - val_loss: 1.2286\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 302ms/step - accuracy: 0.6175 - loss: 1.1460 - val_accuracy: 0.5600 - val_loss: 1.3495\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 337ms/step - accuracy: 0.6550 - loss: 1.0623 - val_accuracy: 0.5800 - val_loss: 1.3482\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 352ms/step - accuracy: 0.6500 - loss: 1.0803 - val_accuracy: 0.5550 - val_loss: 1.4368\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 371ms/step - accuracy: 0.7025 - loss: 1.0058 - val_accuracy: 0.4800 - val_loss: 1.6340\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 315ms/step - accuracy: 0.7050 - loss: 0.9651 - val_accuracy: 0.5700 - val_loss: 1.4298\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001FD709A4F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001FD709A4F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Results: accuracy=0.5650, loss=1.2286, f1=0.5695\n",
      "\n",
      "🔹 Fold 5/5\n",
      "Found 800 images belonging to 5 classes.\n",
      "Found 200 images belonging to 5 classes.\n",
      "Class indices: {'American_Bulldog': 0, 'German_Shorthaired': 1, 'Havanese': 2, 'Maine_Coon': 3, 'Pomeranian': 4}\n",
      "American_Bulldog : 160\n",
      "German_Shorthaired : 160\n",
      "Havanese : 160\n",
      "Maine_Coon : 160\n",
      "Pomeranian : 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 337ms/step - accuracy: 0.2113 - loss: 2.7230 - val_accuracy: 0.2000 - val_loss: 2.0020\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 301ms/step - accuracy: 0.2537 - loss: 1.8280 - val_accuracy: 0.2000 - val_loss: 1.7531\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 303ms/step - accuracy: 0.3150 - loss: 1.6473 - val_accuracy: 0.3000 - val_loss: 1.5988\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 300ms/step - accuracy: 0.3738 - loss: 1.5009 - val_accuracy: 0.3700 - val_loss: 1.4168\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 294ms/step - accuracy: 0.4425 - loss: 1.4442 - val_accuracy: 0.4500 - val_loss: 1.4641\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 296ms/step - accuracy: 0.4725 - loss: 1.3831 - val_accuracy: 0.5250 - val_loss: 1.2985\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 296ms/step - accuracy: 0.4750 - loss: 1.3557 - val_accuracy: 0.5200 - val_loss: 1.3256\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 293ms/step - accuracy: 0.5362 - loss: 1.2619 - val_accuracy: 0.5300 - val_loss: 1.3367\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 295ms/step - accuracy: 0.5763 - loss: 1.1984 - val_accuracy: 0.3950 - val_loss: 1.6218\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 290ms/step - accuracy: 0.5775 - loss: 1.1895 - val_accuracy: 0.5000 - val_loss: 1.3955\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 294ms/step - accuracy: 0.6400 - loss: 1.1004 - val_accuracy: 0.5750 - val_loss: 1.2186\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 312ms/step - accuracy: 0.6150 - loss: 1.1294 - val_accuracy: 0.5050 - val_loss: 1.3676\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 320ms/step - accuracy: 0.6712 - loss: 1.0190 - val_accuracy: 0.5350 - val_loss: 1.4561\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 304ms/step - accuracy: 0.6837 - loss: 0.9760 - val_accuracy: 0.4250 - val_loss: 1.7179\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 312ms/step - accuracy: 0.6837 - loss: 1.0144 - val_accuracy: 0.5750 - val_loss: 1.4460\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 351ms/step - accuracy: 0.7125 - loss: 0.9450 - val_accuracy: 0.5250 - val_loss: 1.5561\n",
      "Fold 5 Results: accuracy=0.5750, loss=1.2186, f1=0.5741\n"
     ]
    }
   ],
   "source": [
    "# === TRAIN & EVALUATE PER FOLD ===\n",
    "for fold in range(1, k_folds + 1):\n",
    "    print(f\"\\n🔹 Fold {fold}/{k_folds}\")\n",
    "    train_dir = base_dir / f\"fold_{fold}\" / \"train\"\n",
    "    val_dir   = base_dir / f\"fold_{fold}\" / \"val\"\n",
    "\n",
    "    # Generators per fold\n",
    "    train_gen = train_datagen.flow_from_directory(\n",
    "        train_dir, target_size=img_size, batch_size=batch_size, class_mode=\"categorical\"\n",
    "    )\n",
    "    val_gen = val_datagen.flow_from_directory(\n",
    "        val_dir, target_size=img_size, batch_size=batch_size, class_mode=\"categorical\", shuffle=False\n",
    "    )\n",
    "\n",
    "    print(\"Class indices:\", train_gen.class_indices)\n",
    "    for cls, idx in train_gen.class_indices.items():\n",
    "        print(cls, \":\", sum(train_gen.classes == idx))\n",
    "\n",
    "\n",
    "    model = build_model()\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_gen,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    val_loss, val_acc = model.evaluate(val_gen, verbose=0)\n",
    "    y_true = val_gen.classes\n",
    "    y_pred = np.argmax(model.predict(val_gen, verbose=0), axis=1)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    print(f\"Fold {fold} Results: accuracy={val_acc:.4f}, loss={val_loss:.4f}, f1={f1:.4f}\")\n",
    "    results.append({'fold': fold, 'accuracy': val_acc, 'loss': val_loss, 'f1': f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715582d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 5-Fold Cross Validation Summary:\n",
      "Accuracy : 0.5160 ± 0.0469\n",
      "F1-Score : 0.5052 ± 0.0574\n",
      "Loss     : 1.3063\n",
      "\n",
      "🔸 Training final model on all data with best configuration...\n",
      "Found 800 images belonging to 5 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "# === AVERAGE METRICS ===\n",
    "accs = [r['accuracy'] for r in results]\n",
    "losses = [r['loss'] for r in results]\n",
    "f1s = [r['f1'] for r in results]\n",
    "\n",
    "avg_acc = np.mean(accs)\n",
    "avg_loss = np.mean(losses)\n",
    "avg_f1 = np.mean(f1s)\n",
    "std_acc = np.std(accs)\n",
    "std_f1 = np.std(f1s)\n",
    "\n",
    "print(\"\\n📊 5-Fold Cross Validation Summary:\")\n",
    "print(f\"Accuracy : {avg_acc:.4f} ± {std_acc:.4f}\")\n",
    "print(f\"F1-Score : {avg_f1:.4f} ± {std_f1:.4f}\")\n",
    "print(f\"Loss     : {avg_loss:.4f}\")\n",
    "\n",
    "# Save metrics\n",
    "os.makedirs(\"checkpoints_exp2\", exist_ok=True)\n",
    "with open(\"checkpoints_exp2/exp2_kfold_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# === FINAL MODEL TRAINING ON FULL DATA ===\n",
    "print(\"\\n🔸 Training final model on all data with best configuration...\")\n",
    "final_train_dir = Path(\"splitted_data_kfold/fold_1/train\")\n",
    "\n",
    "final_train_gen = train_datagen.flow_from_directory(\n",
    "    final_train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False   # keep order for evaluation\n",
    ")\n",
    "\n",
    "final_model = build_model()\n",
    "history_final = final_model.fit(\n",
    "    final_train_gen,\n",
    "    epochs=epochs,\n",
    "    verbose=1\n",
    ")\n",
    "final_model.save(\"checkpoints_exp2/final_model.h5\")\n",
    "\n",
    "# === EVALUATE FINAL MODEL ON FULL TRAINING DATA ===\n",
    "final_loss, final_acc = final_model.evaluate(final_train_gen, verbose=0)\n",
    "y_true_full = final_train_gen.classes\n",
    "y_pred_full = np.argmax(final_model.predict(final_train_gen, verbose=0), axis=1)\n",
    "final_f1 = f1_score(y_true_full, y_pred_full, average='macro')\n",
    "\n",
    "print(\"\\n📘 Final Model (Full Training Data) Results:\")\n",
    "print(f\"Accuracy: {final_acc:.4f}\")\n",
    "print(f\"Loss:     {final_loss:.4f}\")\n",
    "print(f\"F1-Score: {final_f1:.4f}\")\n",
    "\n",
    "# === COMBINE AND SAVE ALL RESULTS ===\n",
    "summary = {\n",
    "    \"cross_validation\": {\n",
    "        \"accuracy_mean\": float(avg_acc),\n",
    "        \"accuracy_std\": float(std_acc),\n",
    "        \"f1_mean\": float(avg_f1),\n",
    "        \"f1_std\": float(std_f1),\n",
    "        \"loss_mean\": float(avg_loss)\n",
    "    },\n",
    "    \"final_model\": {\n",
    "        \"accuracy\": float(final_acc),\n",
    "        \"loss\": float(final_loss),\n",
    "        \"f1_score\": float(final_f1)\n",
    "    },\n",
    "    \"per_fold\": results\n",
    "}\n",
    "\n",
    "os.makedirs(\"checkpoints_exp2\", exist_ok=True)\n",
    "with open(\"checkpoints_exp2/exp2_full_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n✅ Experiment 2 complete. Summary saved to checkpoints_exp2/exp2_full_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
